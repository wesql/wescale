name: Test Docker Build Workflow

on:
#  pull_request:
#    types: [opened, synchronize, reopened]
  workflow_dispatch:

env:
  IMAGE_NAME: wescale_ci_image
  REGISTRY: ghcr.io
  IMAGE_TAG: test-${{ github.sha }}

jobs:
  build-image:
    permissions:
      contents: read
      packages: write
    uses: ./.github/workflows/build_image.yml
    with:
      branch: ${{ github.ref }}
      image_name: ${{ github.repository_owner }}/${{ env.IMAGE_NAME }}
      tags: ${{ env.IMAGE_TAG }}
      registry: ${{ env.REGISTRY }}
      platforms: linux/arm64/v8

  test-cluster:
    needs: build-image
    runs-on: macos-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install kubectl
        run: brew install kubectl

      - name: Install Docker CLI
        run: brew install docker

      - name: Install Colima
        run: |
          brew install qemu
          brew install colima
          LIMACTL_PATH=$(brew --prefix)/bin/limactl
          sudo curl -L -o $LIMACTL_PATH https://github.com/mikekazakov/lima-nohvf/raw/master/limactl && sudo chmod +x $LIMACTL_PATH

      - name: Login to registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ github.token }}

      - name: Start Colima with Kubernetes
        run: |
          if ! colima start --kubernetes --network-address --arch arm64 --vm-type=qemu; then
            echo "Failed to start Colima"
            colima status
            exit 1
          fi

          MAX_ATTEMPTS=20
          ATTEMPT=1
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            if docker info > /dev/null 2>&1 && kubectl cluster-info > /dev/null 2>&1; then
              echo "✅ Docker and Kubernetes are running"
              break
            fi
            echo "Waiting for Docker and Kubernetes to start (attempt $ATTEMPT/$MAX_ATTEMPTS)..."
            sleep 10
            ATTEMPT=$((ATTEMPT + 1))
          done

          if [ $ATTEMPT -gt $MAX_ATTEMPTS ]; then
            echo "❌ Timeout waiting for Docker and Kubernetes"
            docker info || true
            kubectl cluster-info || true
            exit 1
          fi

      - name: Verify Kubernetes is running
        run: |
          kubectl get nodes
          kubectl get pods -A

      - name: Verify and pull test image
        id: verify-image
        run: |
          IMAGE="${{ env.REGISTRY }}/${{ github.repository_owner }}/${{ env.IMAGE_NAME }}"
          TAG="${{ env.IMAGE_TAG }}"

          echo "Waiting for image to be available..."
          MAX_ATTEMPTS=12
          for i in $(seq 1 $MAX_ATTEMPTS); do
            if docker pull $IMAGE:$TAG; then
              echo "✅ Successfully pulled image"
              docker images | grep ${{ env.IMAGE_NAME }}
              exit 0
            fi
            echo "Attempt $i/$MAX_ATTEMPTS: Image not yet available, waiting..."
            sleep 10
          done

          echo "❌ Failed to pull image after $MAX_ATTEMPTS attempts"
          exit 1

      - name: Get Yaml Template
        uses: actions/checkout@v3
        with:
          repository: wesql/deploy
          path: './deploy'

      - name: Init yaml
        run: |
          TAG="test-${{ github.sha }}"
          IMAGE="${{ env.REGISTRY }}/${{ github.repository_owner }}/${{ env.IMAGE_NAME }}"
          sed "s#WESCALE_CI_IMAGE_NAME#$IMAGE#g; s#WESCALE_CI_IMAGE_TAG#$TAG#g" ./deploy/artifact/wescale-ci-template.yaml > ./wescale_cluster.yaml
          echo "$(cat ./wescale_cluster.yaml)"

      - name: Configure AWS CLI
        run: |
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set default.region us-east-2

      - name: Create S3 Bucket
        id: create_bucket
        run: |
          BUCKET_NAME="wescale-$(date +'%Y%m%d%H%M%S')"
          echo "Bucket name: $BUCKET_NAME"
          aws s3 mb s3://$BUCKET_NAME
          echo "bucket_name=$BUCKET_NAME" >> $GITHUB_OUTPUT

      - name: Create ConfigMap and Secret in Kubernetes
        run: |
          export WESQL_OBJECTSTORE_BUCKET=${{ steps.create_bucket.outputs.bucket_name }}
          export WESQL_OBJECTSTORE_REGION=us-east-2
          export WESQL_OBJECTSTORE_ACCESS_KEY=${{ secrets.AWS_ACCESS_KEY_ID }}
          export WESQL_OBJECTSTORE_SECRET_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}

          kubectl apply -f - <<EOF
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: wesql-server-config
          data:
            MYSQL_CUSTOM_CONFIG: |
              [mysqld]
              objectstore_provider=aws
              objectstore_region=${WESQL_OBJECTSTORE_REGION}
              objectstore_bucket=${WESQL_OBJECTSTORE_BUCKET}
              repo_objectstore_id=sysbench
              branch_objectstore_id=main
              datadir=/data/mysql/data
              log-error=/data/mysql/log/mysqld-error.log
              log-bin=binlog
              gtid_mode=ON
              enforce_gtid_consistency=ON
              log_slave_updates=ON
              binlog_format=ROW
              skip_name_resolve=ON
          EOF

          kubectl create secret generic wesql-server-secret \
          --namespace default \
          --type Opaque \
          --from-literal=WESQL_OBJECTSTORE_ACCESS_KEY=${WESQL_OBJECTSTORE_ACCESS_KEY} \
          --from-literal=WESQL_OBJECTSTORE_SECRET_KEY=${WESQL_OBJECTSTORE_SECRET_KEY} \
          --from-literal=MYSQL_ROOT_PASSWORD=passwd

      - name: Generate Cluster YAML File And Create Cluster
        run: |
          kubectl apply -f ./wescale_cluster.yaml

      - name: Wait for WeSQL Server to be ready
        run: |
          kubectl wait --for=condition=available deployment/wesql-vtgate --timeout=3000s
          kubectl port-forward svc/wesql-vtgate-headless 15306:15306 &

          MAX_WAIT=300
          INTERVAL=5
          start_time=$(date +%s)
          
          while true; do
            current_time=$(date +%s)
            elapsed=$((current_time - start_time))
          
            if [ $elapsed -gt $MAX_WAIT ]; then
              echo "Timeout waiting for serving primary tablet after ${MAX_WAIT} seconds"
              exit 1
            fi
          
            output=$(kubectl exec -it $(kubectl get pods -l app.kubernetes.io/name=wesql-vtgate -o jsonpath='{.items[0].metadata.name}') -- mysql -uroot -P15306 -ppasswd -e "show vitess_tablets;" 2>/dev/null)
          
            if [ $? -ne 0 ]; then
              echo "Error executing command, retrying in ${INTERVAL} seconds..."
              sleep $INTERVAL
              continue
            fi
          
            if echo "$output" | grep -q "PRIMARY"; then
              if echo "$output" | grep -q "NOT_SERVING"; then
                echo "Found PRIMARY tablet but it's NOT_SERVING... waiting (${elapsed}s elapsed)"
              else
                if echo "$output" | grep -q "SERVING"; then
                  echo "Primary tablet is now SERVING!"
                  echo "Current tablets status:"
                  echo "$output"
                  exit 0
                fi
              fi
            else
              echo "Waiting for PRIMARY tablet... (${elapsed}s elapsed)"
            fi
          
            sleep $INTERVAL
          done

#      - name: Run tests
#        run: |
#          docker run --rm zyclonite/sysbench oltp_read_write \
#            --tables=5 \
#            --table-size=10000 \
#            --mysql-db=mysql \
#            --mysql-host=host.docker.internal \
#            --mysql-port=15306 \
#            --mysql-user='root' \
#            --mysql-password='' \
#            prepare
#
#          docker run --rm zyclonite/sysbench oltp_read_write --tables=5 --table-size=10000 --mysql-db=mysql --mysql-host=host.docker.internal --mysql-port=15306 --mysql-user='root' --threads=8 --mysql-password='' run

      - name: Clean up
        if: always()
        run: |
          kubectl delete -f ./wescale_cluster.yaml
          echo "Deleting bucket: ${{ steps.create_bucket.outputs.bucket_name }}"
          aws s3 rm s3://${{ steps.create_bucket.outputs.bucket_name }} --recursive
          aws s3 rb s3://${{ steps.create_bucket.outputs.bucket_name }}
          
          TOKEN="${{ github.token }}"
          OWNER="${{ github.repository_owner }}"
          TAG="test-${{ github.sha }}"
          PACKAGE_NAME="${{ env.IMAGE_NAME }}"
          
          echo "Fetching package versions..."
          VERSIONS_JSON=$(curl -s \
            -H "Authorization: Bearer $TOKEN" \
            -H "Accept: application/vnd.github+json" \
            "https://api.github.com/orgs/$OWNER/packages/container/$PACKAGE_NAME/versions")
          
          PACKAGE_VERSION_ID=$(echo "$VERSIONS_JSON" | jq -r ".[] | select(.metadata.container.tags[] | contains(\"$TAG\")) | .id")
          
          if [ ! -z "$PACKAGE_VERSION_ID" ]; then
            echo "Found image version ID: $PACKAGE_VERSION_ID"
          
            DELETE_RESPONSE=$(curl -X DELETE -s -H "Authorization: Bearer $TOKEN" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/orgs/$OWNER/packages/container/$PACKAGE_NAME/versions/$PACKAGE_VERSION_ID")
          
            if [ -z "$DELETE_RESPONSE" ]; then
              echo "✅ Successfully deleted image version: $PACKAGE_VERSION_ID"
            else
              echo "❌ Failed to delete image version. Response: $DELETE_RESPONSE"
              exit 1
            fi
          else
            echo "❌ Could not find image version with tag: $TAG"
            echo "Available versions:"
            echo "$VERSIONS_JSON" | jq -r '.[].names'
            exit 1
          fi